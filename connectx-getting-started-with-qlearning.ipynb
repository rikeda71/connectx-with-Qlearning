{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install kaggle-environments"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install 'kaggle-environments>=0.1.6'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gym  # gym: OpenAI Gymのこと。OpenAIは人工知能系の非営利企業で有名なところ。強化学習のシミュレーション用PF\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make, utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debug/Train your Agent\n## Implement QLearning Model"},{"metadata":{},"cell_type":"markdown","source":"### QTable"},{"metadata":{"trusted":true},"cell_type":"code","source":"class QTable():\n    def __init__(self, actions):\n        self.Q = {}\n        self.actions = actions\n    \n    def get_state_key(self, state):\n        board = state.board[:]\n        board.append(state.mark)\n        state_key = np.array(board).astype(str) \n        return hex(int(''.join(state_key), 3))[2:]        \n        \n    def get_q_values(self, state):\n        # 状態に対して、全actionのQ値のリストを取得\n        state_key = self.get_state_key(state)\n        if state_key not in self.Q.keys(): \n            # 過去にその状態になったことがない場合は0埋め\n            self.Q[state_key] = [0] * len(self.actions)\n        return self.Q[state_key]\n    \n    def update(self, state, action, add_q):\n        # Q値を更新\n        state_key = self.get_state_key(state)\n        self.Q[state_key] = [q + add_q if idx == action else q for idx, q in enumerate(self.Q[state_key])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Agent of  QLearning"},{"metadata":{"trusted":true},"cell_type":"code","source":"class QLearningAgent():\n    def __init__(self, env, epsilon=0.99):\n        self.env = env\n        self.actions = list(range(self.env.configuration.columns))\n        self.q_table = QTable(self.actions)\n        self.epsilon = epsilon\n        self.reward_log = []    \n        \n    def policy(self, state):\n        if np.random.random() < self.epsilon:\n            return choice([c for c in range(len(self.actions)) if state.board[c] == 0])\n        else:\n            q_values = self.q_table.get_q_values(state)\n            selected_items = [q if state.board[idx] == 0 else -1e7 for idx, q in enumerate(q_values)]\n            return int(np.argmax(selected_items))\n        \n    def custom_reward(self, reward, done):\n        if done:\n            if reward == 1: # 勝ち\n                return 20\n            elif reward == 0: # 負け\n                return -20\n            else: # 引き分け\n                return 10\n        else:\n            return -0.05 # 勝負がついてない\n        \n    def train(self, trainer, battle_cnt=100, gamma=0.5, \n              learning_rate=0.1, epsilon_decay_rate=0.99, min_epsilon=0.1):\n        \"\"\"\n        QLearningを実行するためのメソッド\n        \"\"\"\n        \n        for _ in tqdm(range(battle_cnt)):\n            state = trainer.reset() \n            # slow down epsilon\n            self.epsilon = max(min_epsilon, self.epsilon * epsilon_decay_rate) \n            while not env.done:\n                # execute battle\n                action = self.policy(state) \n                next_state, reward, done, info = trainer.step(action)\n                reward = self.custom_reward(reward, done)\n                # Update QTable with gradient\n                gain = reward + gamma * max(self.q_table.get_q_values(next_state))\n                estimate = self.q_table.get_q_values(state)[action]\n                self.q_table.update(state, action, learning_rate * (gain - estimate)) \n                state = next_state\n      \n            self.reward_log.append(reward)                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configs"},{"metadata":{"trusted":true},"cell_type":"code","source":"episode_cnt = 20000  # 強化学習の実行回数\ngamma = 0.5\nlearn_rate = 0.4\nepsilon_decay_rate = 0.9999\nmin_epsilon = 0.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"connectx\", debug=True)\ntrainer = env.train([None, \"random\"])\n# Training\nqa = QLearningAgent(env)\nqa.train(trainer, episode_cnt, gamma, learn_rate, epsilon_decay_rate, min_epsilon)\n\nobservation = trainer.reset()\n\n# ゲーム終了時に得られた報酬の移動平均\nimport seaborn as sns\nsns.set(style='darkgrid')\npd.DataFrame({'Average Reward': qa.reward_log}).rolling(500).mean().plot(figsize=(10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate your Agent"},{"metadata":{},"cell_type":"markdown","source":"# Play your Agent\nClick on any column to place a checker there (\"manually select action\")."},{"metadata":{},"cell_type":"markdown","source":"# Write Submission File\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp_dict_q_table = qa.q_table.Q.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))\n\n# 以下のメソッドを.pyファイルに書き込む\n\"\"\"\ndef my_agent(observation, configuration):\n    from random import choice\n    q_table = str(dict_q_table).replace(' ', '')\n    q_table =  '''{}'''.join(q_table)\n    \n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    return action\n\"\"\"\n    \n\nmy_agent = '''def my_agent(observation, configuration):\n    from random import choice\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n    return action\n    '''\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([my_agent, \"negamax\"])\nenv.render(mode=\"ipython\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}